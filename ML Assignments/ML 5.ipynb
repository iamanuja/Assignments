{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "### Assignment 5 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d8139be",
   "metadata": {},
   "source": [
    "##### 1. What are the key tasks that machine learning entails? What does data pre-processing imply ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc1f8cfa",
   "metadata": {},
   "source": [
    "**Ans:Following are the key machine learning tasks** \n",
    "1. Regression\n",
    "2. Classification\n",
    "3. Clustering\n",
    "4. Transcription\n",
    "5. Machine translation\n",
    "6. Anomaly detection\n",
    "7. Synthesis & sampling\n",
    "8. Estimation of probability density and probability mass function\n",
    "9. Similarity matching\n",
    "10. Co-occurrence grouping\n",
    "11. Causal modeling\n",
    "12. Link profiling\n",
    "\n",
    "Following are the key development phases that are used to solve the different tasks listed above. These form the key phases of the machine learning models’ (MLM) development lifecycle.\n",
    "\n",
    "1. Data gathering\n",
    "2. Data preprocessing\n",
    "3. Exploratory data analysis (EDA)\n",
    "4. Feature engineering including feature creation/extraction, feature selection, dimensionality reduction\n",
    "5. Training machine learning models\n",
    "6. Model / Algorithm selection\n",
    "7. Testing and matching\n",
    "8. Model monitoring\n",
    "9. Model retraining\n",
    "\n",
    "Data preprocessing is required tasks for cleaning the data and making it suitable for a machine learning model which also increases the accuracy and efficiency of a machine learning model.\n",
    "\n",
    "It involves below steps:\n",
    "\n",
    "Getting the dataset-\n",
    "Importing libraries-\n",
    "Importing datasets-\n",
    "Finding Missing Data-\n",
    "Encoding Categorical Data-\n",
    "Splitting dataset into training and test set-\n",
    "Feature scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03e45119",
   "metadata": {},
   "source": [
    "##### 2. Describe quantitative and qualitative data in depth. Make a distinction between the two ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "817c28d0",
   "metadata": {},
   "source": [
    "**Ans:** The Data Type Is Broadly Classified Into:\n",
    "1. Quantitative\n",
    "2. Qualitative\n",
    "\n",
    "**Qualitative Data(Categorical Data)**\n",
    "Qualitative data is a set of information which can not be measured using numbers. It generally consist of words, subjective narratives. Categorical data can take numerical values. For example, maybe we would use 1 for the colour red and 2 for blue. But these numbers don’t have a mathematical meaning. That is, we can’t add them together or take the average.\n",
    "It contains 2 types :\n",
    "**Nominal** : “Nominal” scales could simply be called “labels.”  Note: a sub-type of nominal scale with only two categories (e.g. male/female) is called “dichotomous.”\n",
    "**Ordinal** : “Ordinal” is easy to remember because is sounds like “order” and that’s the key to remember with\n",
    "“ordinal scales”– it is the order that matters, but that’s all you really get from these. \n",
    "\n",
    "**Quantitative data (Numerical Data:)** Numerical data is any data where data points are exact numbers. Statisticians also might call numerical data, quantitative data. This data has meaning as a measurement such as house prices or as a count, such as a number of residential properties in Los Angeles or how many houses sold in the past year.\n",
    "It contains 2 types :\n",
    "**discrete** : Discrete data refers to countable, individualized, and nondivisible figures in statistics. These data points exist only in set increments. Data analysts and statisticians visualize discrete data using bar graphs, line charts, histograms, and pie charts.\n",
    "**continous** : Continuous data is a type of numerical data that refers to the unspecified number of possible measurements between two realistic points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208b7757",
   "metadata": {},
   "source": [
    "##### 3. Create a basic data collection that includes some sample records. Have at least one attribute from each of the machine learning data types ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93cee59a",
   "metadata": {},
   "source": [
    "**Ans:** The following is a basic data collection that includes some sample records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a59b1713",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bccf7ff6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>State Abbreviation</th>\n",
       "      <th>Year</th>\n",
       "      <th>Gender</th>\n",
       "      <th>Education Level of Mother</th>\n",
       "      <th>Education Level Code</th>\n",
       "      <th>Number of Births</th>\n",
       "      <th>Average Age of Mother (years)</th>\n",
       "      <th>Average Birth Weight (g)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>AL</td>\n",
       "      <td>2016</td>\n",
       "      <td>F</td>\n",
       "      <td>8th grade or less</td>\n",
       "      <td>1</td>\n",
       "      <td>1052</td>\n",
       "      <td>27.8</td>\n",
       "      <td>3116.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>AL</td>\n",
       "      <td>2016</td>\n",
       "      <td>F</td>\n",
       "      <td>9th through 12th grade with no diploma</td>\n",
       "      <td>2</td>\n",
       "      <td>3436</td>\n",
       "      <td>24.1</td>\n",
       "      <td>3040.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>AL</td>\n",
       "      <td>2016</td>\n",
       "      <td>F</td>\n",
       "      <td>High school graduate or GED completed</td>\n",
       "      <td>3</td>\n",
       "      <td>8777</td>\n",
       "      <td>25.4</td>\n",
       "      <td>3080.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>AL</td>\n",
       "      <td>2016</td>\n",
       "      <td>F</td>\n",
       "      <td>Some college credit, but not a degree</td>\n",
       "      <td>4</td>\n",
       "      <td>6453</td>\n",
       "      <td>26.7</td>\n",
       "      <td>3121.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Alabama</td>\n",
       "      <td>AL</td>\n",
       "      <td>2016</td>\n",
       "      <td>F</td>\n",
       "      <td>Associate degree (AA, AS)</td>\n",
       "      <td>5</td>\n",
       "      <td>2227</td>\n",
       "      <td>28.9</td>\n",
       "      <td>3174.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5491</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>WY</td>\n",
       "      <td>2021</td>\n",
       "      <td>M</td>\n",
       "      <td>Associate degree (AA, AS)</td>\n",
       "      <td>5</td>\n",
       "      <td>401</td>\n",
       "      <td>29.2</td>\n",
       "      <td>3261.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5492</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>WY</td>\n",
       "      <td>2021</td>\n",
       "      <td>M</td>\n",
       "      <td>Bachelor's degree (BA, AB, BS)</td>\n",
       "      <td>6</td>\n",
       "      <td>657</td>\n",
       "      <td>30.7</td>\n",
       "      <td>3286.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5493</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>WY</td>\n",
       "      <td>2021</td>\n",
       "      <td>M</td>\n",
       "      <td>Master's degree (MA, MS, MEng, MEd, MSW, MBA)</td>\n",
       "      <td>7</td>\n",
       "      <td>261</td>\n",
       "      <td>33.0</td>\n",
       "      <td>3249.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5494</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>WY</td>\n",
       "      <td>2021</td>\n",
       "      <td>M</td>\n",
       "      <td>Doctorate (PhD, EdD) or Professional Degree (M...</td>\n",
       "      <td>8</td>\n",
       "      <td>72</td>\n",
       "      <td>33.3</td>\n",
       "      <td>3262.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5495</th>\n",
       "      <td>Wyoming</td>\n",
       "      <td>WY</td>\n",
       "      <td>2021</td>\n",
       "      <td>M</td>\n",
       "      <td>Unknown or Not Stated</td>\n",
       "      <td>-9</td>\n",
       "      <td>41</td>\n",
       "      <td>29.2</td>\n",
       "      <td>3177.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5496 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        State State Abbreviation  Year Gender  \\\n",
       "0     Alabama                 AL  2016      F   \n",
       "1     Alabama                 AL  2016      F   \n",
       "2     Alabama                 AL  2016      F   \n",
       "3     Alabama                 AL  2016      F   \n",
       "4     Alabama                 AL  2016      F   \n",
       "...       ...                ...   ...    ...   \n",
       "5491  Wyoming                 WY  2021      M   \n",
       "5492  Wyoming                 WY  2021      M   \n",
       "5493  Wyoming                 WY  2021      M   \n",
       "5494  Wyoming                 WY  2021      M   \n",
       "5495  Wyoming                 WY  2021      M   \n",
       "\n",
       "                              Education Level of Mother  Education Level Code  \\\n",
       "0                                     8th grade or less                     1   \n",
       "1                9th through 12th grade with no diploma                     2   \n",
       "2                 High school graduate or GED completed                     3   \n",
       "3                 Some college credit, but not a degree                     4   \n",
       "4                             Associate degree (AA, AS)                     5   \n",
       "...                                                 ...                   ...   \n",
       "5491                          Associate degree (AA, AS)                     5   \n",
       "5492                     Bachelor's degree (BA, AB, BS)                     6   \n",
       "5493      Master's degree (MA, MS, MEng, MEd, MSW, MBA)                     7   \n",
       "5494  Doctorate (PhD, EdD) or Professional Degree (M...                     8   \n",
       "5495                              Unknown or Not Stated                    -9   \n",
       "\n",
       "      Number of Births  Average Age of Mother (years)  \\\n",
       "0                 1052                           27.8   \n",
       "1                 3436                           24.1   \n",
       "2                 8777                           25.4   \n",
       "3                 6453                           26.7   \n",
       "4                 2227                           28.9   \n",
       "...                ...                            ...   \n",
       "5491               401                           29.2   \n",
       "5492               657                           30.7   \n",
       "5493               261                           33.0   \n",
       "5494                72                           33.3   \n",
       "5495                41                           29.2   \n",
       "\n",
       "      Average Birth Weight (g)  \n",
       "0                       3116.9  \n",
       "1                       3040.0  \n",
       "2                       3080.0  \n",
       "3                       3121.9  \n",
       "4                       3174.3  \n",
       "...                        ...  \n",
       "5491                    3261.1  \n",
       "5492                    3286.0  \n",
       "5493                    3249.3  \n",
       "5494                    3262.0  \n",
       "5495                    3177.5  \n",
       "\n",
       "[5496 rows x 9 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\Anuja\\Documents\\us_births_2016_2021.csv\")\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "014f70c7",
   "metadata": {},
   "source": [
    "##### 4. What are the various causes of machine learning data issues? What are the ramifications ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10157eba",
   "metadata": {},
   "source": [
    "**Ans:** Noisy data, dirty data, and incomplete data are the quintessential enemies of ideal Machine Learning. The solution to this conundrum is to take the time to evaluate and scope data with meticulous data governance, data integration, and  data exploration until you get clear data. Ramifications or major issues in machine learning are:\n",
    "1. Five practical issues in machine learning and the business implications Data quality.\n",
    "2. Machine learning systems rely on data.\n",
    "3. The complexity and quality trade-off.\n",
    "4. Sampling bias in data.\n",
    "5. Changing expectations and concept drift.\n",
    "6. Monitoring and maintenance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5c2bfb0",
   "metadata": {},
   "source": [
    "##### 5. Demonstrate various approaches to categorical data exploration with appropriate examples ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0df50ce",
   "metadata": {},
   "source": [
    "**Ans:** Various approaches to categorical data exploration are:\n",
    "1. **Unique value count:** One of the first things which can be useful during data exploration is to see how many unique values\n",
    "are there in categorical columns.\n",
    "2. **Frequency Count:** Frequency count is finding how frequent individual values occur in column.\n",
    "3. **Variance:** Variance gives a good indication how the values are spread.\n",
    "4. **Pareto Analysis:** Pareto analysis is a creative way of focusing on what is important. Pareto 80–20 rule can be effectively used in data exploration.\n",
    "5. **Histogram:** Histogram are one of the data scientists favourite data exploration techniques. It gives information on the range of values in which most of the values fall. It also gives information on whether there is any skew in data.\n",
    "6. **Correlation Heat-map between all numeric columns:** The term correlation refers to a mutual relationship or association between two things.\n",
    "7. **Pearson Correlation and Trend between two numeric columns:** Once you have visualised correlation heat-map , the next step is to see the correlation trend between two specific numeric columns.\n",
    "8. **Outlier overview:** Finding something unusual in data is called Outlier detection (also known as anomaly detection). These outliers represent something unusual, rare , anomaly or something exceptional. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "587d9dd4",
   "metadata": {},
   "source": [
    "##### 6. How would the learning activity be affected if certain variables have missing values? Having said that, what can be done about it ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da1acb3",
   "metadata": {},
   "source": [
    "**Ans:** Even in a Well-Designed & Controlled study, Missing data occurs in almost all research. Missing data can reduce the statistical power of a study and can produce biased estimates, leading to invalid conclusions. \n",
    "\n",
    "- Real-world data collection has its own set of problems, It is often very messy which includes missing data, presence of outliers, unstructured manner, etc. \n",
    "- Before looking for any insights from the data, we have to first perform  preprocessing tasks which then only allow us to use that data for further observation and train our machine learning model. \n",
    "- Missing value in a dataset is a very common phenomenon in the reality. \n",
    "- Missing value correction is required to reduce bias and to produce powerful suitable models.\n",
    "- Most of the algorithms can’t handle missing data, thus you need to act in some way to simply not let your code crash. So, let’s begin with the methods to solve the problem.\n",
    "- Methods for dealing with missing values. The popular methods which are used by the machine learning community to handle the missing value for categorical variables in the dataset are as follows: Delete the observations: If there is a large number of observations in the dataset, where all the classes to be predicted are sufficiently represented in the training data, then try deleting the missing value observations, which would not bring significant change in your feed to your model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3e5411c",
   "metadata": {},
   "source": [
    "##### 7. Describe the various methods for dealing with missing data values in depth ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42f3210",
   "metadata": {},
   "source": [
    "**Ans:** The Various Methods for dealing with missing data values are: \n",
    "1. **Delete the observations:** If there is a large number of observations in the dataset, where all the classes to be predicted are sufficiently represented in the training data, then try deleting the missing value observations, which would not bring significant change in your feed to your model. For Example Implement this method in a given dataset, we can delete the entire row which contains missing values.\n",
    "2. **Replace missing values with the most frequent value:** You can always impute them based on Mode in the case of categorical variables, just make sure you don’t have highly skewed class distributions.\n",
    "3. **Develop a model to predict missing values:** One smart way of doing this could be training a classifier over your columns with missing values as a dependent variable against other features of your data set and trying to impute based on the newly trained classifier."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "438aafab",
   "metadata": {},
   "source": [
    "##### 8. What are the various data pre-processing techniques? Explain dimensionality reduction and function selection in a few words ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06291f51",
   "metadata": {},
   "source": [
    "**Ans:** The Various Data Pre-Processing Techniques are:\n",
    "-  **Data Cleaning:**  The data can have many irrelevant and missing parts. To handle this part, data cleaning is done. It  involves handling of missing data, noisy data etc.  \n",
    "    - **Missing Data:**  This situation arises when some data is missing in the data. It can be handled in various ways.  Some of them are: \n",
    "        - **Ignore the tuples:**  This approach is suitable only when the dataset we have is quite large and multiple values are missing within a tuple.  \n",
    "        - **Fill the Missing values:** There are various ways to do this task. You can choose to fill the missing values manually, by attribute mean or the most probable value.\n",
    "    - **Noisy Data:** Noisy data is a meaningless data that can’t be interpreted by machines.It can be generated due to  faulty data collection, data entry errors etc. It can be handled in following ways : \n",
    "        - **Binning Method:** This method works on sorted data in order to smooth it. The whole data is divided into segments of equal size and then various methods are performed to complete  the task. Each segmented is handled separately. One can replace all data in a  segment by its mean or boundary values can be used to complete the task.  \n",
    "        - **Regression:** Here data can be made smooth by fitting it to a regression function.The regression used may be linear (having one independent variable) or multiple (having multiple  independent variables). \n",
    "        - **Clustering:** This approach groups the similar data in a cluster. The outliers may be undetected or it will fall outside the clusters. \n",
    "        \n",
    "     \n",
    "-  **Data Reduction:**  Since data mining is a technique that is used to handle huge amount of data. While working with  huge volume of data, analysis became harder in such cases. In order to get rid of this, we uses data  reduction technique. It aims to increase the storage efficiency and reduce data storage and analysis costs. The various steps to data reduction are: \n",
    "    - **Data Cube Aggregation:** Aggregation operation is applied to data for the construction of the data cube.  \n",
    "    - **Attribute Subset Selection:** The highly relevant attributes should be used, rest all can be discarded. For performing  attribute selection, one can use level of significance and p- value of the attribute.the attribute  having p-value greater than significance level can be discarded. \n",
    "    - **Numerosity Reduction:** This enable to store the model of data instead of whole data, for example: Regression Models. \n",
    "    - **Dimensionality Reduction:** This reduce the size of data by encoding mechanisms.It can be lossy or lossless. If after reconstruction from compressed data, original data can be retrieved, such reduction are called lossless reduction else it is called lossy reduction. The two effective methods of dimensionality reduction are:Wavelet transforms and PCA (Principal Component Analysis). \n",
    " \n",
    "Feature selection is simply selecting and excluding given features without changing them. Dimensionality reduction transforms features into a lower dimension."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74b54cf9",
   "metadata": {},
   "source": [
    "##### 9.Make brief notes on of the following ?\n",
    "1. What is the IQR? What criteria are used to assess it?\n",
    "2. Describe the various components of a box plot in detail? When will the lower whisker    surpass the upper whisker in length? How can box plots be used to identify outliers?"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ffc60b0a",
   "metadata": {},
   "source": [
    "**Ans:** The following is the brief notes on the following topics:\n",
    "\n",
    "- **What is the IQR? What criteria are used to assess it?**\n",
    "    - Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1. \n",
    "    - Q3 is the third quartile of the data, i.e., to say 75% of the data lies between minimum and Q3. \n",
    "    - The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR.\n",
    "\n",
    "\n",
    "- **Describe the various components of a box plot in detail? When will the lower whisker surpass  the upper whisker in length? How can box plots be used to identify outliers?**\n",
    "    - minimum is the minimum value in the dataset\n",
    "    - maximum is the maximum value in the dataset.\n",
    "    - So the difference between the two tells us about the range of dataset.\n",
    "    - The median is the median (or centre point), also called second quartile, of the data (resulting from the fact that the data is ordered).\n",
    "    - Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1.\n",
    "    - Q3 is the third quartile of the data, i.e., to say 75% \n",
    "    - When the data is left skewed, lower whisker will be longer than upper whisker. \n",
    "    - To detect the outliers this method is used, we define a new range, let’s call it decision range, and any  data point lying outside this range is considered as outlier and is accordingly dealt with. The range is  as given below:\n",
    "    **Lower Bound: (Q1 - 1.5 * IQR)\n",
    "    Upper Bound: (Q3 + 1.5 * IQR)**\n",
    "    - The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR. \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b6f9818",
   "metadata": {},
   "source": [
    "##### 10. Make brief notes on any two of the following ?\n",
    "1. Data collected at regular intervals\n",
    "2. The gap between the quartiles\n",
    "3. Use a cross-tab"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e40e1aa",
   "metadata": {},
   "source": [
    "**Ans:** The following are the breif notes about:\n",
    "- **Data collected at regular intervals:**\n",
    "    - Interval data is one of the two types of discrete data. \n",
    "    - An example of interval data is the data collected on a  thermometer—its gradation or markings are equidistant. \n",
    "    - Unlike ordinal data, interval data always take numerical values where the distance between two points on the scale is standardised and equal.\n",
    "    \n",
    "    \n",
    "- **The gap between the quartiles:** \n",
    "    - Q1 is the first quartile of the data, i.e., to say 25% of the data lies between minimum and Q1. \n",
    "    - Q3 is the third quartile of the data, i.e., to say 75% of the data lies between minimum and Q3. \n",
    "    - The difference between Q3 and Q1 is called the Inter-Quartile Range or IQR."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae262dc0",
   "metadata": {},
   "source": [
    "##### 11. Make a comparison between ?\n",
    "1. Data with nominal and ordinal values\n",
    "2. Histogram and box plot\n",
    "3. The average and median"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0a9b43a",
   "metadata": {},
   "source": [
    "**Ans:** The following are the breif notes about:\n",
    "- **The average and median:** \n",
    "    - The mean (informally, the “average“) is found by adding all of the numbers together and dividing  by the number of items in the set: 10 + 10 + 20 + 40 + 70 / 5 = 30. The median is found by ordering  the set from lowest to highest and finding the exact middle. The median is just the middle number:  20\n",
    "    \n",
    "    \n",
    "    \n",
    "- **Histogram and box plot:**\n",
    "    - Histograms and box plots are very similar in that they both help to visualize and describe numeric data. Although histograms are better in determining the underlying distribution of the data, box plots allow you to compare multiple data sets better than histograms as they are less detailed and take up less space."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
