{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b2aa57f2",
   "metadata": {},
   "source": [
    "## Assignment 12 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "111120c6",
   "metadata": {},
   "source": [
    "##### 1. What is prior probability ? Give an example ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "104d6217",
   "metadata": {},
   "source": [
    "**Ans:** The prior probability of an event will be revised as new data or information becomes available, to produce a more accurate measure of a potential outcome. That revised probability becomes the posterior probability and is calculated using Bayes' theorem. In statistical terms, the posterior probability is the probability of event A occurring given that event B has occurred.\n",
    "\n",
    "For example, three acres of land have the labels A, B, and C. One acre has reserves of oil below its surface, while the other two do not. The prior probability of oil being found on acre C is one third, or 0.333. But if a drilling test is conducted on acre B, and the results indicate that no oil is present at the location, then the posterior probability of oil being found on acres A and C become 0.5, as each acre has one out of two chances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ae857d4",
   "metadata": {},
   "source": [
    "##### 2. What is posterior probability ? Give an example ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c74bcef",
   "metadata": {},
   "source": [
    "**Ans:**: Posterior probability is a revised probability that takes into account new available information. The prior probability of oil being found on acre C is one third, or 0.333. But if a drilling test is conducted on acre B, and the results indicate that no oil is present at the location, then the posterior probability of oil being found on acres A and C become 0.5, as each acre has one out of two chances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c981ac1e",
   "metadata": {},
   "source": [
    "##### 3. What is likelihood probability ? Give an example ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8618f0f7",
   "metadata": {},
   "source": [
    "**Ans:** he term \"probability\" refers to the possibility of something happening.The term Likelihood refers to the process of determining the best data distribution given a specific situation in the data. When calculating the probability of a given outcome, you assume the model's parameters are reliable.However, when you calculate the likelihood, you’re attempting to determine whether the parameters in a model can be trusted based on the sample data you have observed.\n",
    " **likelihood refers to The probability of falling under a specific category or class.**\n",
    "**Example Scenario**\n",
    "Suppose you have an unbiased coin. If you flip the coin, the probability of getting head and a tail is equal, which is 0.5\n",
    "\n",
    "Now suppose the same coin is tossed 50 times, and it shows heads only 14 times. You would assume that the likelihood of the unbiased coin is very low. If the coin were fair, it would have shown heads and tails the same number of times.\n",
    "\n",
    "When calculating the probability of coin getting heads, you assume that P(head) = 0.5\n",
    "\n",
    "However, when calculating the likelihood, you are trying to find if the model parameter (p = 0.5) is correctly specified or not. \n",
    "\n",
    "The fact that a coin only lands on heads 14 times out of 50 makes you highly suspicious that the true probability of a coin landing on heads on a given toss is p = 0.5. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07ab2a64",
   "metadata": {},
   "source": [
    "##### 4. What is Naïve Bayes classifier ? Why is it named so ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51ea164d",
   "metadata": {},
   "source": [
    "**Ans:** Naive Bayes is a simple and powerful algorithm for predictive modeling. It is a probabilistic classifier, which means it predicts on the basis of the probability of an object.Some popular examples of Naïve Bayes Algorithm are spam filtration, Sentimental analysis, and classifying articles.\n",
    "\n",
    "**Why is it called Naïve Bayes?**\n",
    "The Naïve Bayes algorithm is comprised of two words Naïve and Bayes, Which can be described as:\n",
    "\n",
    "**Naïve:** It is called Naïve because it assumes that the occurrence of a certain feature is independent of the occurrence of other features. Such as if the fruit is identified on the bases of color, shape, and taste, then red, spherical, and sweet fruit is recognized as an apple. Hence each feature individually contributes to identify that it is an apple without depending on each other.\n",
    "**Bayes:** It is called Bayes because it depends on the principle of Bayes' Theorem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be2162e",
   "metadata": {},
   "source": [
    "##### 5. What is optimal Bayes classifier ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7abae5",
   "metadata": {},
   "source": [
    "**Ans:** The Bayes Optimal Classifier is a probabilistic model that makes the most probable prediction for a new example.\n",
    "This model is also referred to as the Bayes optimal learner, the Bayes classifier, Bayes optimal decision boundary, or the Bayes optimal discriminant function.\n",
    "It is described using the Bayes Theorem that provides a principled way for calculating a conditional probability. It is also closely related to the Maximum a Posteriori: a probabilistic framework referred to as MAP that finds the most probable hypothesis for a training dataset.\n",
    "\n",
    "In practice, the Bayes Optimal Classifier is computationally expensive, if not intractable to calculate, and instead, simplifications such as the Gibbs algorithm and Naive Bayes can be used to approximate the outcome.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a96163dd",
   "metadata": {},
   "source": [
    "##### 6. Write any two features of Bayesian learning methods ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09004071",
   "metadata": {},
   "source": [
    "**Ans:** Each observed training example can incrementally decrease or increase the estimated probability that a hypothesis is correct.– This provides a more flexible approach to learning than algorithms that completely eliminate a hypothesis if it is found to be inconsistent with any single example.\n",
    "\n",
    "\n",
    "•Prior knowledge can be combined with observed data to determine the final probability of a hypothesis. In Bayesian learning, prior knowledge is provided by asserting–  a prior probability for each candidate hypothesis, and–  a probability distribution over observed data for each possible hypothesis.\n",
    "\n",
    "\n",
    "•Bayesian methods can accommodate hypotheses that make probabilistic predictions\n",
    "\n",
    "\n",
    "•New instances can be classified by combining the predictions of multiple hypotheses, weighted by theirprobabilities.\n",
    "\n",
    "\n",
    "•Even in cases where Bayesian methods prove computationally intractable, they can provide a standard of optimal decision making against which other practical methods can be measured"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3b649e",
   "metadata": {},
   "source": [
    "##### 7. Define the concept of consistent learners ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eba4091",
   "metadata": {},
   "source": [
    "**Ans:** **Consistent Learners:** A consistent learning algorithm is simply required to output a hypothesis that is consistent\n",
    "with all the training data provided to it. So far, we have not imposed any requirement on\n",
    "the hypothesis class H. This notion of consistency is closely related to the empirical risk\n",
    "minimisation principle in the machine learning literature, where the risk is defined using the\n",
    "zero-one loss.. \n",
    "\n",
    "a learning algorithm is a consistent learner if it commits zero errors over the training examples\n",
    "every consistent learner outputs a MAP hypothesis if 1) we assume a uniform prior probability distribution over H and if 2) we assume a deterministic noise free training data.\n",
    "Find-S & Candidate-Elimination output a MAP hypotheses\n",
    "Bayesian perspective can be used to characterize learning algorithms even if they do not explicitly manipulate probabilities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54020cf8",
   "metadata": {},
   "source": [
    "##### 8. Write any two strengths of Bayes classifier ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28211a96",
   "metadata": {},
   "source": [
    "**Ans:** \n",
    "1. Naïve Bayes is one of the fast and easy ML algorithms to predict a class of datasets.\n",
    "2. It can be used for Binary as well as Multi-class Classifications.\n",
    "3. It performs well in Multi-class predictions as compared to the other Algorithms.\n",
    "4. It is the most popular choice for text classification problems.\n",
    "5. This algorithm works quickly and can save a lot of time.\n",
    "6. Naive Bayes is suitable for solving multi-class prediction problems. \n",
    "7. If its assumption of the independence of features holds true, it can perform better than other models and requires much less training data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db681d1",
   "metadata": {},
   "source": [
    "##### 9. Write any two weaknesses of Bayes classifier ?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7646cf4",
   "metadata": {},
   "source": [
    "**Ans:** \n",
    "1. If your test data set has a categorical variable of a category that wasn’t present in the training data set, the Naive Bayes model will assign it zero probability and won’t be able to make any predictions in this regard. This phenomenon is called ‘Zero Frequency,’ and you’ll have to use a smoothing technique to solve this problem.\n",
    "2. This algorithm is also notorious as a lousy estimator. So, you shouldn’t take the probability outputs of ‘predict_proba’ too seriously. \n",
    "3. It assumes that all the features are independent. While it might sound great in theory, in real life, you’ll hardly find a set of independent features. \n",
    "4. The greatest weakness of the naïve Bayes classifier is that it relies on an often-faulty assumption of equally important and independent features which results in biased posterior probabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b686929",
   "metadata": {},
   "source": [
    "##### 10. Explain how Naïve Bayes classifier is used for:\n",
    "1. Text classification\n",
    "2. Spam filtering\n",
    "3. Market sentiment analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8447d36",
   "metadata": {},
   "source": [
    "**Ans:** Navie Bayes Classifier is used for:\n",
    "- **Text classification:**\n",
    "The Naive Bayes classifier is a simple classifier that classifies based on probabilities of events. It is the applied  commonly to text classification. With the training set, we can train a Naive Bayes classifier which we can use to automaticall categorize a new sentence.\n",
    "- **Spam filtering:**        \n",
    "Naive Bayes classifiers work by correlating the use of tokens (typically words, or sometimes other things), with spam and non-spam e-mails and then using Bayes' theorem to calculate a probability that an email is or is not spam.  It is one of the oldest ways of doing spam filtering, with roots in the 1990s.\n",
    "- **Market sentiment analysis:**    \n",
    "Market Sentiment analysis is a field dedicated to extracting subjective emotions and feelings from text. One common use of sentiment analysis is to figure out if a text expresses negative or positive feelings. Naive Bayes is a popular algorithm for classifying text."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
